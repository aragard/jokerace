<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Arbitrum DAO Sensemaking through JokeRace Analysis          </title>
    <meta charset="utf-8" />
    <meta name="author" content="Omni Analytics Group" />
    <script src="libs/header-attrs-2.24/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="classroom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Arbitrum DAO Sensemaking through JokeRace Analysis <br> <br> <br> <br> <br>
]
.author[
### Omni Analytics Group
]

---




## Sidenote

If you're ready to get started right away, you can skip straight to the "Getting Started" Section. The tutorial consists of two main parts: "Part 1: Topic Analysis" and " Part 2: High-level Text Analysis" (ie. "n-grams" and "POS Tagging"). For intermediate students (ie. ideally students who have already worked on at least one other case study), feel free to skip directly to these sections.
---

## Introduction

This tutorial will walk you through the process we took in order to produce the text analysis for the Arbitrum JokeRaces. Our aim in this case study is to perform some text analysis on responses to something called a "JokeRace" (defined in the next section). We captured all of the submissions ([dataset available here](https://docs.google.com/spreadsheets/d/17Evor8Xl5_Ks9OpifEQxARS6MfsvU1sUTM54tS1JK8c/edit#gid=705342398)) so that we could analyze them to find common themes across the responses by getting a feel for the amount of noise, perform a topic analysis to better understand what was said and perform some basic text analysis to extract the main thoughts for each race without having to read every single post.

---

## Arbitrum

**Arbitrum** is a Layer 2 scaling solution for the Ethereum blockchain that aims to address the limitations of the Ethereum network, such as slow transaction speeds and high fees (particularly during times of high activity). It is designed to improve the speed of transactions, increase scalability, and reduce costs. The layer 2 Arbitrum released their governance token a few months back. Now that they have both have products (principally the L2 Arbitrum Rollup) and a token (\$ARB). Plurality labs proposed one of two grant proposals designed to help with "sense-making" for the DAO, ie. creating an overarching mission. Our objective is to assist the DAO in setting an agenda that reflects the goals, mission and vision of the community. For supplementary reading on the DAO and the products they are building, [A gentle introduction to Arbitrum](https://docs.arbitrum.io/intro) is a good starting place while [Arbitrum DAO: A conceptual overview](https://docs.arbitrum.foundation/concepts/arbitrum-dao) has more information on the DAO. We encourage you to skim these readings so that you can derive more meaningful interpretations from your results later on.

---

## JokeRaces

A **JokeRace** is a survey platform that incentives participation by paying people for their opinions. People are prompted to respond to some query, for which the JokeRace then follows a schedule where submissions are open, and then closed while people vote. For each JokeRace, approximately, 5,000 submissions were collected for a total of 20,000 responses.

There are four (4) JokeRaces, each asking as a separate question about what the ARB DAO should do in the future:


| JokeRace Contest Name                                              | Contract Address                           |
|--------------------------------------------------------------------|--------------------------------------------|
| ArbitrumDAO Short-term Strategic Priorities (Reduce Friction)      | 0xbf47BDA4b172daf321148197700cBED04dbe0D58 |
| ArbitrumDAO Long-term Strategic Priorities (Growth and Innovation) | 0x5D4e25fA847430Bf1974637f5bA8CB09D0B94ec7 |
| ArbitrumDAO Northstar Strategic Framework: Vision                  | 0x0d4C05e4BaE5eE625aADC35479Cc0b140DDF95D4 |
| ArbitrumDAO Northstar Strategic Framework: Mission                 | 0x5a207fA8e1136303Fd5232E200Ca30042c45c3B6 |

---

## Data Collection

Our initial job was to collect the dataset ([available here](https://docs.google.com/spreadsheets/d/17Evor8Xl5_Ks9OpifEQxARS6MfsvU1sUTM54tS1JK8c/edit#gid=705342398)). We found the contract for each jokerace and then queried each of the transactions, for which we pulled out the data from each. We queried Dune Dashboards to extract all of the transactions for the 4 contracts in question:

``` sql
SELECT * FROM arbitrum.logs
WHERE contract_address IN 
(0xbf47BDA4b172daf321148197700cBED04dbe0D58,
0x5D4e25fA847430Bf1974637f5bA8CB09D0B94ec7,
0x0d4C05e4BaE5eE625aADC35479Cc0b140DDF95D4,
0x5a207fA8e1136303Fd5232E200Ca30042c45c3B6)
AND topic0 =
0xcd423cc1203c0af96b9b3d68d73b3064a69de2d14450bb7181c5e5df2132b358
```

---

## Data Preview

Below are the headings of the csv columns:

| Heading     | Description                            |
|-------------|----------------------------------------|
| `BlockTime` | When the Submission hit the blockchain |
| `Contract`  | Which JokeRace was this submission for |
| `Address`   | The wallet address of the submitter    |
| `Slug`      | An ID                                  |
| `URL`       | URL of the submission                  |
| `IsImage`   | Whether it had an image                |
| `Content`   | The text submission                    |

---

## Getting Started

### Libraries for Topic Analysis

In this tutorial, we will use the following R packages/ libraries to perform topic analysis:


```r
# Load packages
library(knitr)
library(kableExtra)
library(DT)
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(flextable)
library(stringi)
```

---


### Libraries for High-level Text Analysis

We use the following R packages/ libraries for our high-level text-analysis:


```r
# Load packages
library(tidytext)
library(dplyr)
library(purrr)
library(udpipe)
library(tidyr)
library(stringr)
library(readr)
```

If you need additional guidance getting set up, please refer to our earlier tutorials on getting using R Studio and R.

---

## Loading Data

The first step (after loading the libraries) is to load the data into the current R session.


```r
# Load Data
propfinal &lt;- readRDS("./propfinal.RDS")
```

The data is provided as an RDS file called "propfinal.RDS". RDS is an R binary storage format that can be read directly into the current session using the `readRDS()` function call.

You can view the file in a spreadsheet application like Excel or use `head(propfinal)` to preview the data. the contents of the CSV look like below:
&lt;img src="./images/propfinal.png" alt="Propfinal CSV Screenshot" width="100%" height="100%"&gt;

---

## Stopwords

Next, we can download common English "stopwords", save them in a vector and add our own stopwords such as "arbitrumdao", "arbitrum", etc. These are words that we expect to see during our analysis. You can add new stopwords to further fine tune the results and remove noise that doesn't add new informational value during your interpretation of the outputs.


```r
english_stopwords &lt;- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
english_stopwords &lt;- c(english_stopwords,c("arbitrumdao","arbitrum","project","arb","dao", "daos")) # Add new stop words here
```

---

# Part 1: Topic Analysis

---

## Topic Analysis

**Topic Analysis** consists of methods (ex. LDA, NMF, etc.) that are used to identify and extract topics or themes from a corpus (a collection of documents or text data). It's a valuable tool for organizing, summarizing, and understanding large volumes of text. Latent Dirichlet Allocation (LDA) is a common technique for topic modelling and the method used in this case study. We performed Topic Analysis using LDA on each respective JokeRace to ascertain the common themes within each race.

A more detailed explanation of Topic Analysis methods is beyond the scope of this tutorial but [here](https://ladal.edu.au/topicmodels.html) is a good supplementary resource on Topic Analysis using LDA.

---
## Choosing Number of Topics

We will create a dataframe of dataframes called "contestdf" that allows us to do two (2) main tasks:

1. Keep track of which jokerace the topics belong to

2. keep track of the number of topics we are using for the analysis

In this tutorial, we utilize a newer method of selecting the topics involving a mini-max approach across two metrics. In the subsequent slides, we will briefly walkthrough the plots for these metrics. It is a pretty simple method to interpret so once you read the first graph, feel welcome to interpret the other subsequent plots and see if your intuition matches our answers.

---

### ArbitrumDAO Short-term Strategic Priorities (Reduce Friction)

**Contract: 0xbf47bda4b172daf321148197700cbed04dbe0d58**

There were 1463 submissions and the DTM had a vocab of 763.

&lt;img src="./images/minmax_1.png" alt="Minmax Results 1" width="50%" height="50%"&gt;

#### Interpretation
 To determine the optimal number of topics we simply choose a point that minimizes the top plot while maximizing the bottom plot so here we select 4 topics (If you're unfamiliar with DTMs refer to the "Compute DTM" section).

---

### ArbitrumDAO Long-term Strategic Priorities (Growth and Innovation)

**Contract: 0x5d4e25fa847430bf1974637f5ba8cb09d0b94ec7**

There were 1582 submissions and the DTM had a vocab of 818.

&lt;img src="./images/minmax_2.png" alt="Minmax Results 2" width="50%" height="50%"&gt;

#### Interpretation

Similarly, the optimal number of topics in this case is 5 topics since this choice minimizes the top plot while maximising the bottom plot.

---

### ArbitrumDAO Northstar Strategic Framework: Vision

**Contract: 0x0d4c05e4bae5ee625aadc35479cc0b140ddf95d4**

There were 1519 submissions and the DTM had a vocab of 746.

&lt;img src="./images/minmax_3.png" alt="Minmax Results 3" width="50%" height="50%"&gt;

#### Interpretation

Based on the plots, we can select 7 or 10 topics so we will create two separate dataframes (ie. V7 and V10) to keep track of both separately so we can compare the results easily.

---

### ArbitrumDAO Northstar Strategic Framework: Mission

**Contract: 0x5a207fa8e1136303fd5232e200ca30042c45c3b6**

There were 1506 submissions and the DTM had a vocab of 744.

&lt;img src="./images/minmax_4.png" alt="Minmax Results 4" width="50%" height="50%"&gt;

#### Interpretation

Based on the plots we can select 11 or 15 topics. These are saved in the M11 and M15 dataframes in contestdf.

---

## Contest Dataframe

Now we are ready to create a dataframe called "contestdf" that contains six (6) simple dataframes that are identifiable by their "slugs" (ie. RF4, GI5, V7, etc.). Each dataframe contains a contract "Address", the "Name" of a given JokeRace, a "Slug" used to uniquely identify each dataframe, and the number of "Topics".


```r
contestdf &lt;- do.call(rbind,list(
                                  data.frame(Address = "0xbf47bda4b172daf321148197700cbed04dbe0d58",Name = "Reduce Friction",Slug = "RF4",Topics = 4),
                                  data.frame(Address = "0x5d4e25fa847430bf1974637f5ba8cb09d0b94ec7",Name = "Growth and Innovation",Slug = "GI5",Topics = 5),
                                  data.frame(Address = "0x0d4c05e4bae5ee625aadc35479cc0b140ddf95d4",Name = "Vision",Slug = "V7",Topics = 7),
                                  data.frame(Address = "0x0d4c05e4bae5ee625aadc35479cc0b140ddf95d4",Name = "Vision",Slug = "V10",Topics = 10),
                                  data.frame(Address = "0x5a207fa8e1136303fd5232e200ca30042c45c3b6",Name = "Mission",Slug = "M11",Topics = 11),
                                  data.frame(Address = "0x5a207fa8e1136303fd5232e200ca30042c45c3b6",Name = "Mission",Slug = "M15",Topics = 15)
                            )
              )
```

---

## Contest Dataframe Slugs

The six (6) dataframes contained within contestdf are summarized below:

| Slug  | Topic of that Submission                  |
|-------|-------------------------------------------|
| `RF4` | Reduce Friction Race using 4 topics       |
| `GI5` | Growth and Innovation Race using 5 topics |
| `V7`  | Vision Race using 7 topics                |
| `V10` | Vision Race using 10 topics               |
| `M11` | Mission Race using 11 topics              |
| `M15` | Mission Race using 15 topics              |

---

## Save Topic Analysis Results

Since we effectively have to perform topic analysis six times (once for each dataframe RF4, GI5, etc.), we will use a main for-loop and save the `results` in a variable located outside of the body of this for-loop (see next slide to see how the loop is set up).


```r
# Save Topic Modelling Results
results &lt;- as.data.frame(matrix(1:50))
names(results) &lt;- "TermIdx"
```

Here we are creating one-column matrix containing 50 rows, converting it to a one-column dataframe and saving it in a variable called `results`. In the second line, we name the column heading, `TermIdx`.

---

## Setting up Main For-Loop for Topic Analysis

We set up the main for-loop to loop through each of the 6 rows in `contestdf` to keep track of which jokerace we are currently working on (ie. when idx has the value of '1', it is referring to RF4, when idx has the value of '2', we are working on GI5, etc.).


```r
for(idx in 1:nrow(contestdf))
{
  # Topic Analysis code will go here
}
```

---

## Data Cleaning

If you inspect the propfinal dataframe with a close eye, you will notice that the data still needs cleaning. We need to remove HTML tags, stem words and duplicates as well as filter out low quality submissions to get high quality results. Below is the code that we used to creata a corpus while cleaning the data. We will split up this code and explain it in the next couple slides.


```r
textdata &lt;- propfinal$ContentParsed[propfinal$Contract==contestdf$Address[idx]]
textdata &lt;- textdata[Encoding(textdata)!="bytes"]
textdata &lt;- textdata[nchar(textdata)&gt;40]
textdata &lt;- textdata[!(textdata %in% names(which(table(textdata)&gt;1)))]
textdata &lt;- data.frame(Contest = contestdf$Name[idx],doc_id = 1:length(textdata),text = textdata)
corpus &lt;- Corpus(DataframeSource(textdata))
```

---

### Extracting Textdata from Propfinal Dataframe

The first lines of code that we will add to the body of the loop are for extracting the text data from the "Content" column of `propfinal`. Recall that `propfinal` contains all the data while `contestdf` allows us to keep track of the respective JokeRace contracts and the number of topics to perform topic analysis. In the first line, we take all of the text data concerning this contract and store in an appropriately named variable named `textdata`. In the second line, we encode this data as "bytes".


```r
textdata &lt;- propfinal$ContentParsed[propfinal$Contract==contestdf$Address[idx]]
textdata &lt;- textdata[Encoding(textdata)!="bytes"]
```

---

### Filtering Low Quality Submissions and Removing Duplicates

For the purposes of thresholding, we used this rubric to get a feel for the character length required to create a meaningful contribution:

```         
This is 21 characters
This is 43 characters This is 43 characters
This is 84 characters This is 84 characters This is 84 characters This is 84 characters
```

We determined that we could safely filter out some of the noise by removing submissions containing less than 40 characters since it is difficult to say anything meaningful with that level of brevity. This occurs in the first line in the following code snippet. In the second line, we remove duplicates to further reduce noise. Rudimentary analysis using Excel revealed that nearly 50% of the responses were duplicates.


```r
textdata &lt;- textdata[nchar(textdata)&gt;40]
textdata &lt;- textdata[!(textdata %in% names(which(table(textdata)&gt;1)))]
```

---

### Creating Corpus

As mentioned earlier, the **corpus** contains the body of textual data that will be used for analysis. This code creates a dataframe containing three (3) columns called 'Contest', 'doc_id' and 'text'. This dataframe is then used to create the corpus that we will use to perform our topic analysis. The `Corpus` function from the `tm` text mining package does the work involved in converting the `textdata` into a `corpus`.


```r
textdata &lt;- data.frame(Contest = contestdf$Name[idx],doc_id = 1:length(textdata),text = textdata)
corpus &lt;- Corpus(DataframeSource(textdata))
```
  
---

## Preprocessing

Below is code to create a pre-processing chain to pre-process the text data stored in the corpus. Each step in the chain applies a specific text pre-processing operation from the `tm_map` function to clean and prepare the text data. The operations are self-explanatory. We are performing the following operations on the corpus (in order):

1. Converting everything to lowercase
2. Removing English stopwords
3. Removing punctuation without changing hypenated words (ie. preserving the dash in words like "high-speed") 
4. Removing numbers
5. Stemming ie. reducing words to their stems (base or root forms)
6. Trimming extra whitespace


```r
  # Pre-processing chain
  processedCorpus &lt;- tm_map(corpus, content_transformer(tolower))
  processedCorpus &lt;- tm_map(processedCorpus, removeWords, english_stopwords)
  processedCorpus &lt;- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
  processedCorpus &lt;- tm_map(processedCorpus, removeNumbers)
  processedCorpus &lt;- tm_map(processedCorpus, stemDocument, language = "en")
  processedCorpus &lt;- tm_map(processedCorpus, stripWhitespace)
```

The result of these operations creates clean and standardized text data that is now suitable for analysis.
---

## Compute DTM

A **document-term matrix (DTM)** is a numerical representation of text data where rows represent documents (ie. pieces of text) and columns represent terms (ie. words or tokens). Each cell in the matrix contains the frequency of a term's occurrence in a particular document. In the code below, the first line excludes words appearing in the text less than 5 times from appearing in the DTM. The second line creates the `DTM` from the `processedCorpus` and the last line prints the dimensions of the resulting `DTM` matrix in the console.


```r
  # compute document term matrix with terms &gt;= minimumFrequency
  minimumFrequency &lt;- 5
  DTM &lt;- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
  # have a look at the number of documents and terms in the matrix
  dim(DTM)
```

```
## [1] 1514  743
```

---

## Removing Empty Rows

Empty rows in the DTM can cause issues in the LDA algorithm so we must remove them.


```r
  # due to vocabulary pruning, we have empty rows in our DTM
  # LDA does not like this. So we remove those docs from the
  # DTM and the metadata
  sel_idx &lt;- slam::row_sums(DTM) &gt; 0
  DTM &lt;- DTM[sel_idx, ]
  textdata &lt;- textdata[sel_idx, ]
```

---

## Latent Dirichlet Allocation (LDA) Algorithm

LDA is a topic modelling technique that is performed on a DTM created from a corpus. The first line extracts the number of topics from the current dataframe. Recall that each row in `contestdf` contains a dataframe with a contract "Address", the "Name" of a given JokeRace, a "Slug" used to uniquely identify each dataframe, and the number of "Topics". The second line sets the seed for the random number generator. Setting the seed ensures that the LDA results are reproducible. In the third line, the LDA model is computed using the Gibbs sampling method using 5000 iterations and displaying progress outputs every 25 iterations.


```r
# number of topics
  K &lt;- contestdf$Topics[idx]
  # set random number generator seed
  set.seed(9161)
  # compute the LDA model, inference via 5000 iterations of Gibbs sampling
  topicModel &lt;- LDA(DTM, K, method="Gibbs", control=list(iter = 5000, verbose = 25))
```

---

## Examine and Store LDA Results

The first line calculates the posterior distributions of topics for the documents in the LDA model. The second line stores the temporary results in `result_t`. The third line assigns column names to the `result_t` dataframe. The column headings are created based on the slug, and the index of each topic. An example of a heading is "RF4_Topic_1". This naming scheme allow us to associate each word in a given column with specific topics and contests. For RF4, there will be 4 columns in total. For GI5, there will be 5 columns in total, etc. Line 4 appends the columns in `result_t` to `results`. Line 5 prints the value `idx` representing the current iteration so that we can have some feedback in the console when the loop is running for debugging purposes.


```r
# have a look at some of the results (posterior distributions)
  tmResult &lt;- posterior(topicModel)
  result_t &lt;- as.data.frame(terms(topicModel, 50))
  names(result_t) &lt;- paste0(contestdf$Slug[idx],"_Topic_",1:contestdf$Topics[idx])
  results &lt;- cbind(results,result_t)
  message(idx)
```

---

## Final Body of For-Loop

Combining all of the topic analysis code should result in the following code within the body of the for-loop.


```r
for(idx in 1:nrow(contestdf))
{
  textdata &lt;- propfinal$ContentParsed[propfinal$Contract==contestdf$Address[idx]]
  textdata &lt;- textdata[Encoding(textdata)!="bytes"]
  textdata &lt;- textdata[nchar(textdata)&gt;40]
  textdata &lt;- textdata[!(textdata %in% names(which(table(textdata)&gt;1)))]
  textdata &lt;- data.frame(Contest = contestdf$Name[idx],doc_id = 1:length(textdata),text = textdata)
  corpus &lt;- Corpus(DataframeSource(textdata))

  ########################################################################
  ## Process Text for Topic Modelling
  ########################################################################

  # Pre-processing chain
  processedCorpus &lt;- tm_map(corpus, content_transformer(tolower))
  processedCorpus &lt;- tm_map(processedCorpus, removeWords, english_stopwords)
  processedCorpus &lt;- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
  processedCorpus &lt;- tm_map(processedCorpus, removeNumbers)
  processedCorpus &lt;- tm_map(processedCorpus, stemDocument, language = "en")
  processedCorpus &lt;- tm_map(processedCorpus, stripWhitespace)
```

---

## Final Body of For-Loop (cont'd 1)


```r
  ########################################################################
  ## Topic Modelling
  ########################################################################

  # compute document term matrix with terms &gt;= minimumFrequency
  minimumFrequency &lt;- 5
  DTM &lt;- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
  # have a look at the number of documents and terms in the matrix
  dim(DTM)

  # due to vocabulary pruning, we have empty rows in our DTM
  # LDA does not like this. So we remove those docs from the
  # DTM and the metadata
  sel_idx &lt;- slam::row_sums(DTM) &gt; 0
  DTM &lt;- DTM[sel_idx, ]
  textdata &lt;- textdata[sel_idx, ]
  
  # number of topics
  K &lt;- contestdf$Topics[idx]
  # set random number generator seed
  set.seed(9161)
  # compute the LDA model, inference via 1000 iterations of Gibbs sampling
  topicModel &lt;- LDA(DTM, K, method="Gibbs", control=list(iter = 5000, verbose = 25))
```

---

## Final Body of For-Loop (cont'd 2)

```r
  # have a look at some of the results (posterior distributions)
  tmResult &lt;- posterior(topicModel)
  result_t &lt;- as.data.frame(terms(topicModel, 50))
  names(result_t) &lt;- paste0(contestdf$Slug[idx],"_Topic_",1:contestdf$Topics[idx])
  results &lt;- cbind(results,result_t)
  message(idx)
}
```

---

## Create Master CSV containing Topics

Outside of the body of the for-loop,the `results` are saved in a csv named "TopicTerms.csv" that is stored in the same directory.

The csv should look like this:
&lt;img src="./images/topics.png" alt="Topic CSV Screenshot" width="100%" height="100%"&gt;

Put this code outside the body of the main for-loop and run the script in Rstudio and view the csv to see the results of your topic analysis. 


```r
# Output Topics
write_csv(results,"./TopicTerms.csv")
```

---

# Part 2: High-level Text Analysis

## Setting up Main For-Loop for Ngrams and POS Tagging

We will generate n-grams and perform POS tagging on each of the four (4) contests. We set up the main for-loop to loop through each of the 4 unique contests in `contestdf` (ie. RF4, GI5, V7 and M11). Since we are not performing topic analysis in this section we can skip V10 and M15 (ie. rows 4 and 6 in `contestdf`).


```r
for(idx in 1:nrow(contestdf))
{
  if (idx != 4 &amp;&amp; idx != 6 &amp;&amp; idx != nrow(contestdf))
    {
    # Text Analysis will go here
    }
}
```

---

## n-grams

An **ngram** is just a number indicating the number of words in a sequence (ie. 1-gram is 1 word, 2 grams are 2 words, 3 grams are 3 word sequences, etc.). It is a continguous sequence of n items (ie. characters, words, etc.) extracted from a given text or sentence. We used the top n-grams under each JokeRace to perform a high-level analysis. For example, looking at trigrams (3-grams) for growth and innovation, the phrase "long term strategic", "layer 2 scaling" and "regular security audits" were some of the most frequenctly occurring, suggesting that's where Growth and Innovation should focus ie. scaling but in a secure manner (see image below).

&lt;img src="./images/GI_trigrams.png" alt="GI Trigrams" width="15%" height="15%"&gt;

---

### Top 1-gram

This code uses the `unnest_tokens` function from the `tidytext` package to create 1-grams (unigrams or single words) from `textdata`. It creates a column called `ngram` that containts the 1-grams. We filter out columns that contain missing values and split the ngrams into separate columns. For 1-grams this step isn't strictly necessary because the ngrams are single words but for consistency we include it here. Next, we do some cleaning to remove non-alphanumeric characters, empty rows and common English stop words. The `count` function is used to count the frequency of the words in each column and the `top_n` selects the top 20 1-grams based on their frequency.


```r
# Top 20 unigrams
  top_20_unigrams &lt;- textdata %&gt;%
    unnest_tokens(ngram, text, token = "ngrams", n = 1) %&gt;%
    filter(!is.na(ngram))%&gt;%
    separate(ngram, c("word1"), sep = " ")%&gt;%
    mutate(word1 = str_replace_all(word1, "[^[:alnum:]]", "")) %&gt;%
    filter(word1 != "") %&gt;%
    filter(!word1 %in% english_stopwords) %&gt;%
    count(word1, sort = TRUE)%&gt;%
    top_n(20)
```

We use similar code to create , 2-grams, 3-grams and 4-grams (see next 3 slides).

---

### Top 2-grams


```r
# Top 20 bigrams
top_20_bigrams &lt;- textdata %&gt;%
  unnest_tokens(gram, text, token = "ngrams", n = 2) %&gt;%
  filter(!is.na(gram))%&gt;%
  separate(gram, c("word1", "word2"), sep = " ")%&gt;%
  mutate(word1 = str_replace_all(word1, "[^[:alnum:]]", ""),
         word2 = str_replace_all(word2, "[^[:alnum:]]", "")) %&gt;%
  filter(word1 != "" &amp; word2 != "") %&gt;%
  filter(!word1 %in% english_stopwords) %&gt;%
  filter(!word2 %in% english_stopwords)%&gt;%
  count(word1, word2, sort = TRUE)%&gt;%
  top_n(20)
```

---

### Top 3-grams


```r
# Top 20 trigrams
top_20_trigrams &lt;- textdata %&gt;%
unnest_tokens(gram, text, token = "ngrams", n = 3) %&gt;%
filter(!is.na(gram))%&gt;%
separate(gram, c("word1", "word2", "word3"), sep = " ")%&gt;%
mutate(word1 = str_replace_all(word1, "[^[:alnum:]]", ""),
       word2 = str_replace_all(word2, "[^[:alnum:]]", ""),
       word3 = str_replace_all(word3, "[^[:alnum:]]", "")) %&gt;%
filter(word1 != "" &amp; word2 != "" &amp; word3 != "") %&gt;%
filter(!word1 %in% english_stopwords) %&gt;%
filter(!word2 %in% english_stopwords)%&gt;%
filter(!word3 %in% english_stopwords)%&gt;%
count(word1, word2, word3, sort = TRUE)%&gt;%
top_n(20)
```
  
---

### Top 4-grams


```r
# Top 20 fourgrams
top_20_fourgrams &lt;- textdata %&gt;%
unnest_tokens(gram, text, token = "ngrams", n = 4) %&gt;%
filter(!is.na(gram))%&gt;%
separate(gram, c("word1", "word2", "word3", "word4"), sep = " ")%&gt;%
mutate(word1 = str_replace_all(word1, "[^[:alnum:]]", ""),
       word2 = str_replace_all(word2, "[^[:alnum:]]", ""),
       word3 = str_replace_all(word3, "[^[:alnum:]]", ""),
       word4 = str_replace_all(word4, "[^[:alnum:]]", "")) %&gt;%
filter(word1 != "" &amp; word2 != "" &amp; word3 != "" &amp; word4 != "") %&gt;%
filter(!word1 %in% english_stopwords) %&gt;%
filter(!word2 %in% english_stopwords)%&gt;%
filter(!word3 %in% english_stopwords)%&gt;%
filter(!word4 %in% english_stopwords)%&gt;%
count(word1, word2, word3, word4, sort = TRUE)%&gt;%
top_n(20)
```

---

## POS Tagging
**Parts of Speech (POS)** tagging involves determing the POS (ie. nouns, verbs, adjectives, adverbs, etc.) of each word in a sentence or text.

We performed POS tagging to identify the top nouns and the top verbs. We used nouns as a proxy for what *things* are important and verbs as a proxy for what *actions* are important.

For instance, looking at "Reduce Friction races:

&lt;img src="./images/pos.png" alt="POS" width="25%" height="25%"&gt;

We see that "community" and "users" are big focuses based on the comments left by DAO members while they want to "ensure", "create", "provide" and "decentralize" some of the actions that should be taken.

---

### Tokenise the Text Data

Tokenisation involves breaking text into individual words or tokens. We use the `unnest_tokens` function to tokenise the `textdata` before we remove non-alphanumeric characters, empty rows and English stopwords.


```r
# Tokenise text data and preprocess
  tokens &lt;- unnest_tokens(textdata, word, text) %&gt;%
    mutate(word = str_replace_all(word, "[^[:alnum:]]", "")) %&gt;%
    filter(word != "") %&gt;%
    filter(!word %in% english_stopwords)
```

---

### Load Model

We will use the `udpipe` package for POS tagging by downloading a pre-trained language model for English called "english-ewt". Place this code outside of the body of the main for-loop to avoid it being re-downloaded during every iteration.


```r
# Load a model for 'udpipe'
udmodel &lt;- udpipe_download_model(language = "english-ewt")
udmodel &lt;- udpipe_load_model(file = udmodel$file_model)
```

---

### Tag Tokens with Part of Speech

The following code uses the pre-trained model to annotate  `tokens` (the tokenised text dataset), convert the annotations to a dataframe and join these annotations back to `tokens`.

```r
# Use the model to annotate the 'word' column
annotations &lt;- udpipe_annotate(udmodel, x = tokens$word)

# Convert the output to a data frame
annotations_df &lt;- as.data.frame(annotations)

# Join the annotations back to the original 'tokens' data frame
tokens &lt;- tokens %&gt;%
  left_join(annotations_df, by = c("word" = "token"), relationship = "many-to-many")
```

---

## Top 20 Verbs

Let's generate the top verbs:


```r
# Top 20 verbs
top_verbs &lt;- tokens %&gt;%
filter(upos %in% c("VERB")) %&gt;%
select(word) %&gt;%
count(word, sort = TRUE) %&gt;%
top_n(20)
```
 
---

## Top 20 Nouns

And the top nouns:


```r
# Top 20 nouns
top_nouns &lt;- tokens %&gt;%
filter(upos %in% c("NOUN")) %&gt;%
select(word)%&gt;%
count(word, sort = TRUE) %&gt;%
top_n(20)
```

---

## Output CSVs

Now are we are ready to output CSVs containing the n-grams, top nouns and top verbs:

```r
# Create folder if it doesn't exist
dir.create("./ngrams", showWarnings = FALSE)

# Ngrams
write_csv(top_20_unigrams, paste0("./ngrams/", contestdf$Slug[idx], "_top_unigrams.csv"))
write_csv(top_20_bigrams, paste0("./ngrams/", contestdf$Slug[idx], "_top_bigrams.csv"))
write_csv(top_20_trigrams, paste0("./ngrams/", contestdf$Slug[idx], "_top_trigrams.csv"))
write_csv(top_20_fourgrams, paste0("./ngrams/", contestdf$Slug[idx], "_top_fourgrams.csv"))

# Create folder if it doesn't exist
dir.create("./pos", showWarnings = FALSE)

# Parts of Speech
write_csv(top_verbs, paste0("./pos/", contestdf$Slug[idx], "_top_verbs.csv"))
write_csv(top_nouns, paste0("./pos/", contestdf$Slug[idx], "_top_nouns.csv"))
}
```

---

## Conclusion

Congratulations! You just performed text analysis including Topic Modelling, n-grams and POS tagging on the Arbitrum DAO JokeRace contest. For your next steps, review the slides on Topic Modelling, n-grams and POS tagging for examples of how to interpret the final results, see if you can come up with your own interpretations.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  bottom: 1em;
  left: 1em;
  width: 60px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
